1) Find-S
from sklearn import datasets
import pandas as pd

iris = datasets.load_iris()
df = pd.DataFrame(iris.data)
df["y"] = iris.target

# Discretize numeric values into 3 bins
for i in range(4):
    df[i] = pd.cut(df[i], 3, labels=["low", "med", "high"])

# Initialize S as the first positive example
S = list(df[df.y == 0].iloc[0, :4])

for _, r in df[df.y == 0].iterrows():
    for i in range(4):
        if S[i] != r[i]:
            S[i] = "?"

print("Final S:", S)

-------------------------------------------------------------------------------------------------------
2)Candidate 
from sklearn import datasets
import pandas as pd

iris = datasets.load_iris()
df = pd.DataFrame(iris.data)
df["y"] = iris.target

for i in range(4):
    df[i] = pd.cut(df[i], 3, labels=["low", "med", "high"])

S = ['0'] * 4
G = [['?'] * 4]

for _, r in df.iterrows():
    if r["y"] == 0:
        for i in range(4):
            if S[i] == '0':
                S[i] = r[i]
            elif S[i] != r[i]:
                S[i] = "?"

print("Most Specific Hypothesis S:", S)
print("Most General Hypothesis G:", G[0])

---------------------------------------------------------------------------------------
3)ID-3:-

from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.metrics import confusion_matrix, accuracy_score, roc_curve, auc
from sklearn.preprocessing import label_binarize
import matplotlib.pyplot as plt

iris = datasets.load_iris()
Xtr, Xte, ytr, yte = train_test_split(iris.data, iris.target, test_size=0.3, random_state=1)

model = DecisionTreeClassifier(criterion="entropy").fit(Xtr, ytr)
yp = model.predict(Xte)

print("Confusion Matrix:\n", confusion_matrix(yte, yp))
print("Accuracy:", round(accuracy_score(yte, yp) * 100, 2), "%")

plot_tree(model, filled=True, feature_names=iris.feature_names, class_names=iris.target_names)
plt.show()

# ROC curves
yb = label_binarize(yte, classes=[0, 1, 2])
proba = model.predict_proba(Xte)

for i, name in enumerate(["Setosa", "Versicolor", "Virginica"]):
    f, t, _ = roc_curve(yb[:, i], proba[:, i])
    plt.plot(f, t, label=f"{name} (AUC={auc(f, t):.2f})")

plt.plot([0, 1], [0, 1], '--')
plt.legend()
plt.title("ROC Curve - ID3")
plt.show()
------------------------------------------------------------------------------

4)Backpropagation

from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import confusion_matrix, accuracy_score

iris = datasets.load_iris()
X, y = iris.data, iris.target

Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.3, random_state=1)

model = MLPClassifier(hidden_layer_sizes=(6,), activation='relu', solver='adam', max_iter=1000)
model.fit(Xtr, ytr)

yp = model.predict(Xte)

print("Confusion Matrix:\n", confusion_matrix(yte, yp))
print("Accuracy:", round(accuracy_score(yte, yp) * 100, 2), "%")

-----------------------------------------------------------------------------------------------------------

5)Naive Bayes Classifier:-
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score
from sklearn.datasets import load_iris

iris = load_iris()
X = pd.DataFrame(iris.data, columns=iris.feature_names)
y = pd.Series(iris.target)

Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.3, random_state=1)

model = GaussianNB().fit(Xtr, ytr)
yp = model.predict(Xte)

print("Accuracy:", accuracy_score(yte, yp))
print("Predictions:", yp)
---------------------------------------------------------------------------------------------------------------

6)Bayesian Network:-
from pgmpy.models import BayesianNetwork
from pgmpy.estimators import MaximumLikelihoodEstimator
from sklearn import datasets
import pandas as pd

iris = datasets.load_iris()
df = pd.DataFrame(iris.data, columns=iris.feature_names)
df["target"] = iris.target

model = BayesianNetwork([
    ("sepal length (cm)", "target"),
    ("sepal width (cm)", "target"),
    ("petal length (cm)", "target"),
    ("petal width (cm)", "target")
])

model.fit(df, estimator=MaximumLikelihoodEstimator)

test = df.iloc[:5].drop("target", axis=1)

print("Predictions:\n", model.predict(test))
--------------------------------------------------------------------------------------------------------

7)K-Nearest Neighbors:-

from sklearn import datasets
from sklearn.neighbors import KNeighborsClassifier
import matplotlib.pyplot as plt

iris = datasets.load_iris()
X, y = iris.data, iris.target

acc = []
for k in range(1, 11):
    model = KNeighborsClassifier(n_neighbors=k).fit(X, y)
    acc.append(model.score(X, y))

plt.plot(range(1, 11), acc, marker='o')
plt.title("K vs Accuracy")
plt.show()

best_k = acc.index(max(acc)) + 1
model = KNeighborsClassifier(n_neighbors=best_k).fit(X, y)
yp = model.predict(X)

print("Correct:", [(y[i], yp[i]) for i in range(len(y)) if y[i] == yp[i]])
print("Wrong:", [(y[i], yp[i]) for i in range(len(y)) if y[i] != yp[i]])

-------------------------------------------------------------------------------------------------

8)Locally Weighted Regression:-

from sklearn import datasets
from sklearn.neighbors import KNeighborsClassifier
import matplotlib.pyplot as plt

iris = datasets.load_iris()
X, y = iris.data, iris.target

acc = []
for k in range(1, 11):
    model = KNeighborsClassifier(n_neighbors=k).fit(X, y)
    acc.append(model.score(X, y))

plt.plot(range(1, 11), acc, marker='o')
plt.title("K vs Accuracy")
plt.show()

best_k = acc.index(max(acc)) + 1
model = KNeighborsClassifier(n_neighbors=best_k).fit(X, y)
yp = model.predict(X)

print("Correct:", [(y[i], yp[i]) for i in range(len(y)) if y[i] == yp[i]])
print("Wrong:", [(y[i], yp[i]) for i in range(len(y)) if y[i] != yp[i]])

--------------------------------------------------------------------------------------------------

